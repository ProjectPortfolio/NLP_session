{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VcCl7tcS2dw"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWwA0fubS7bi",
        "outputId": "0bf524e0-a447-4af6-fbfa-6df665861235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "nltk.download(\"popular\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeroNSfVTEFj"
      },
      "source": [
        "#Tokenization :\n",
        "#1)Word Tokenization :By using word_tokenize from nltk.tokenize word tokenization is done.\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyiuG-0KTiyQ"
      },
      "source": [
        "from nltk.tokenize import word_tokenize as wt\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxSmOo-rTyD2",
        "outputId": "8c9ae47b-3bdb-46c7-d414-d91a7eda475e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text=\"I'm Persuing Mtech from TIT CSE.Along with mtech doing training on ML and DL \"\n",
        "print(wt(text))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'m\", 'Persuing', 'Mtech', 'from', 'TIT', 'CSE.Along', 'with', 'mtech', 'doing', 'training', 'on', 'ML', 'and', 'DL']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvi75helUPTV"
      },
      "source": [
        "#2. Sentence Tokenization : Use sent_tokenize method of nltk.tokenize to tokenize sentences."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90EBtbU6UcfQ"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize as st\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CIp66KlUgGA",
        "outputId": "8bac76fd-c1ae-4405-9a53-10b810b1e2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(st(text))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"I'm Persuing Mtech from TIT CSE.Along with mtech doing training on ML and DL\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou4zASBHUgg1"
      },
      "source": [
        "#Stop Words\n",
        "#Stop words are words that are filtered out before or after the natural language data (text) are processed. While “stop words” typically refers to the most common words in a language, all-natural language processing tools don't use a single universal list of stop words.\n",
        "#Use predefined stopwords in nltk.corpus package"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9RMOVX2UvgX"
      },
      "source": [
        "from nltk.corpus import stopwords as sw\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnXwQbDgU1tv"
      },
      "source": [
        "text_paragraph = \"o know deep learning object detection well, as a series of objection detection approaches, if there is enough time, it is better to read R-CNN, Fast R-CNN and Faster R-CNN in order, to know the evolution of objection detection, especially why region proposal network (RPN) is existed in this approach. I suggest to read my reviews about them if interested\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD_ASCH8U_11"
      },
      "source": [
        "# Now store all the stopwords of english language in one set.\n",
        "stop_words = set(sw.words('english'))\n",
        "\n",
        "# Tokenize the words in the paragraph.\n",
        "word_tokens = wt(text_paragraph)\n",
        "\n",
        "# Creating a list to store the filtered words other than stopwords\n",
        "filter_words = []\n",
        "# Filtering the tokenized words\n",
        "for word in word_tokens:\n",
        "  if word not in stop_words:\n",
        "    filter_words.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRHGefATVmGb",
        "outputId": "b092a7d3-3497-403d-9383-1b89c10a9470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(filter_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['know', 'deep', 'learning', 'object', 'detection', 'well', ',', 'series', 'objection', 'detection', 'approaches', ',', 'enough', 'time', ',', 'better', 'read', 'R-CNN', ',', 'Fast', 'R-CNN', 'Faster', 'R-CNN', 'order', ',', 'know', 'evolution', 'objection', 'detection', ',', 'especially', 'region', 'proposal', 'network', '(', 'RPN', ')', 'existed', 'approach', '.', 'I', 'suggest', 'read', 'reviews', 'interested']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcEtLdCLVrvj",
        "outputId": "d609f7f0-6315-4645-aa56-5040617ed6c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['o', 'know', 'deep', 'learning', 'object', 'detection', 'well', ',', 'as', 'a', 'series', 'of', 'objection', 'detection', 'approaches', ',', 'if', 'there', 'is', 'enough', 'time', ',', 'it', 'is', 'better', 'to', 'read', 'R-CNN', ',', 'Fast', 'R-CNN', 'and', 'Faster', 'R-CNN', 'in', 'order', ',', 'to', 'know', 'the', 'evolution', 'of', 'objection', 'detection', ',', 'especially', 'why', 'region', 'proposal', 'network', '(', 'RPN', ')', 'is', 'existed', 'in', 'this', 'approach', '.', 'I', 'suggest', 'to', 'read', 'my', 'reviews', 'about', 'them', 'if', 'interested']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YcnJbLqV1Ea"
      },
      "source": [
        "#Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
        "#Use PorterStemmer of porter class to perform this task."
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsRGWpRtV_K0",
        "outputId": "d16509ac-bf59-4a72-aa18-d4e9c8aee404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Create the object of PorterStemmer class\n",
        "port_stem = PorterStemmer()\n",
        "\n",
        "# Use the filtered words and print the final steeming result.\n",
        "for word in filter_words:\n",
        "  print((word, port_stem.stem(word)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('know', 'know')\n",
            "('deep', 'deep')\n",
            "('learning', 'learn')\n",
            "('object', 'object')\n",
            "('detection', 'detect')\n",
            "('well', 'well')\n",
            "(',', ',')\n",
            "('series', 'seri')\n",
            "('objection', 'object')\n",
            "('detection', 'detect')\n",
            "('approaches', 'approach')\n",
            "(',', ',')\n",
            "('enough', 'enough')\n",
            "('time', 'time')\n",
            "(',', ',')\n",
            "('better', 'better')\n",
            "('read', 'read')\n",
            "('R-CNN', 'r-cnn')\n",
            "(',', ',')\n",
            "('Fast', 'fast')\n",
            "('R-CNN', 'r-cnn')\n",
            "('Faster', 'faster')\n",
            "('R-CNN', 'r-cnn')\n",
            "('order', 'order')\n",
            "(',', ',')\n",
            "('know', 'know')\n",
            "('evolution', 'evolut')\n",
            "('objection', 'object')\n",
            "('detection', 'detect')\n",
            "(',', ',')\n",
            "('especially', 'especi')\n",
            "('region', 'region')\n",
            "('proposal', 'propos')\n",
            "('network', 'network')\n",
            "('(', '(')\n",
            "('RPN', 'rpn')\n",
            "(')', ')')\n",
            "('existed', 'exist')\n",
            "('approach', 'approach')\n",
            "('.', '.')\n",
            "('I', 'I')\n",
            "('suggest', 'suggest')\n",
            "('read', 'read')\n",
            "('reviews', 'review')\n",
            "('interested', 'interest')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4duMvD0kWDPz"
      },
      "source": [
        "#Lemmatization\n",
        "#Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
        "#Use WordNetLemmatizer of nltk.stem to perform lemmatization."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OZziJtQWHsx",
        "outputId": "67176e40-6cf7-4a4a-c8e2-d1ccb28404ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create the object of WordNetLammatizer class.\n",
        "wordNetLemm = WordNetLemmatizer()\n",
        "\n",
        "# Now lemmatize the filtered words.\n",
        "for word in filter_words:\n",
        "  print((word, wordNetLemm.lemmatize(word)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('know', 'know')\n",
            "('deep', 'deep')\n",
            "('learning', 'learning')\n",
            "('object', 'object')\n",
            "('detection', 'detection')\n",
            "('well', 'well')\n",
            "(',', ',')\n",
            "('series', 'series')\n",
            "('objection', 'objection')\n",
            "('detection', 'detection')\n",
            "('approaches', 'approach')\n",
            "(',', ',')\n",
            "('enough', 'enough')\n",
            "('time', 'time')\n",
            "(',', ',')\n",
            "('better', 'better')\n",
            "('read', 'read')\n",
            "('R-CNN', 'R-CNN')\n",
            "(',', ',')\n",
            "('Fast', 'Fast')\n",
            "('R-CNN', 'R-CNN')\n",
            "('Faster', 'Faster')\n",
            "('R-CNN', 'R-CNN')\n",
            "('order', 'order')\n",
            "(',', ',')\n",
            "('know', 'know')\n",
            "('evolution', 'evolution')\n",
            "('objection', 'objection')\n",
            "('detection', 'detection')\n",
            "(',', ',')\n",
            "('especially', 'especially')\n",
            "('region', 'region')\n",
            "('proposal', 'proposal')\n",
            "('network', 'network')\n",
            "('(', '(')\n",
            "('RPN', 'RPN')\n",
            "(')', ')')\n",
            "('existed', 'existed')\n",
            "('approach', 'approach')\n",
            "('.', '.')\n",
            "('I', 'I')\n",
            "('suggest', 'suggest')\n",
            "('read', 'read')\n",
            "('reviews', 'review')\n",
            "('interested', 'interested')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd5_kFMxYkvz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}